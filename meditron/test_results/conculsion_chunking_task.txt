The chunking task involves splitting up the input EC section into smaller chunks that the model can process without exceeding the context length. Here the average size of an example response is used as a buffer so that the input + output should remain below the context length.

Using this methodology, the model is able to make better sense of the desired output. However, for the trials 1-6 that were executed using this methodology, only trial 1 showed promising results for chunks 0 (1451 tokens / 71%) and 1 (1383 tokens / 68%). The others were considered failures and thus the continuation of the remaining trials was stopped early. With trial 1, chunk 2 (1377 tokens), the desired output format was preserved, but the contents were incorrect. So the hypothesis about token size is still inconclusive.

To account for the possibility that the prompt was still too large, a zero-shot prompt was used on trial 1 after the inital pass. The average prompt size after chunking this zero-shot prompt was 720 tokens (35%). The tradeoff with this approach is that the model loses prior examples. This tradeoff appears to be significant as the model failed to comprehend the desired output and failed to capture any of the requested elements.

Input token size under a certain limit (around 70%) is inconclusive as trial 1 shows. The limit for these tests is:
    Input Size + Expected Example Prompt Size < Context Length - Expected Output Size
Here as long as the inputs on the lefthand side of the equation are less than the expected output size (with respect to the maximum length), then the Input is used. Otherwise the Input is split in half and the equation is computed again.
    
Hypothesis: If input token size is inconclusive, but preserving an example within the prompt is conclusive, then reducing the input size towards the minimum required input may yield better results. Otherwise, the input token size remains inconclusive. Adjusting the original chunking equation to test for an arbitrary limit under 70% gives:
    Input Size + Expected Example Prompt Size < Context Length * Arbitrary Limit Under 70%
Accounting for expected output size is not a concern since the output is capped at only 256 tokens, 12% of the total window, and now we have created a output buffer of at least 30%.
The results after running trials 1 and 2 with smaller chunking strategy of 60% of context length as the target input size show that the model is able to better understand the desired output formats. The outputs in trial 1 for the small chunking strategy resulted in 87% correct output format vs 75% in the larger chunks. For trial 2 the difference was much more noticeable with 87% vs 0% output format correctness for small vs large chunks, respectively. However, the accuracy of the output still suffers. While the reduced input size weighs more heavily on the model's ability to follow the templated output, it is unclear if the reduced input size contributes to better NER accuracy.