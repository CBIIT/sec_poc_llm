{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34dddb4-6838-45ad-a805-2836af7ff6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce5468334154161a21d7cb962053d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2434fa8e-0ebb-4b60-aa18-d305531a0ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    # stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=256,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    "    do_sample=True,\n",
    "    # streamer = transformers.TextStreamer(tokenizer)\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c14e5e4-8aa0-4140-9b5e-7cf26677dffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from prompt_examples.single_criterion_examples import prompt, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159c99a8-f29f-44e0-a738-2977f72ec7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are in the role of an abstractor who will analyze an eligibility criterion for a clinical trial and extract the relevant entities as described below.\n",
      "Original Text: the original text of the criterion\n",
      "Disease/Condition: If the criterion contains a disease or condition name it by its canonical name\n",
      "Procedure: If the criterion contains a therapeutic procedure name it by its canonical name\n",
      "Drug:  If the criterion contains a therapeutic drug name it by its canonical name\n",
      "Biomarker:  If the criterion contains a biomarker name it by its canonical name\n",
      "Computable Rule: Translate the criteria into a logical expression that could be interpreted programmatically\n",
      "\n",
      "Criterion:\n",
      "    Subject must not have any affected lymph nodes or metastatic disease\n",
      "\n",
      "Output:\n",
      "    Original Text: Subject must not have any affected lymph nodes or metastatic disease\n",
      "    Disease/Condition: Lymph node-Metastases, Metastatic Disease\n",
      "    Procedure: none\n",
      "    Drug: none\n",
      "    Biomarker: none\n",
      "    Computable Rule: LK MTS == False, MTS == False\n",
      "\n",
      "Criterion:\n",
      "    Patient must have undergone complete surgical resection of their stage IIA, IIB, IIIA or IIIB non-squamous or squamous b NSCLC per American Joint Committee on Cancer (AJCC) 8th edition and have had negative margins. N3 disease is not allowed.\n",
      "\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(criterion=\"\"\"Patient must have undergone complete surgical resection of their stage IIA, IIB, IIIA or IIIB non-squamous or squamous b NSCLC per American Joint Committee on Cancer (AJCC) 8th edition and have had negative margins. N3 disease is not allowed.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17ef896-2a6a-4cf3-acdc-6b036b01a36f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  avg prompt: 121\n",
      "  min prompt: 66\n",
      "  max prompt: 227\n",
      "avg response: 89\n"
     ]
    }
   ],
   "source": [
    "from token_counting import *\n",
    "globalize_token_metrics(examples)\n",
    "print('  avg prompt:', AVG_PROMPT_LEN)\n",
    "print('  min prompt:', MIN_PROMPT_LEN)\n",
    "print('  max prompt:', MAX_PROMPT_LEN)\n",
    "print('avg response:', AVG_RES_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c02c6e5-e477-47be-a50c-5509444e292c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chunking import parse_file_with_pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78706696-5748-4432-a898-82f9140f55d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "import time\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "langchain.debug = False\n",
    "langchain.verbose = False\n",
    "\n",
    "n = '01'\n",
    "folder = f'test_results_final/trial{n}'\n",
    "folderp = Path(folder)\n",
    "logfile = folderp / \"outputs.log\"\n",
    "logger.add(logfile, colorize=False, enqueue=True)\n",
    "handler = langchain.callbacks.FileCallbackHandler(logfile)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=False)\n",
    "\n",
    "criterions = parse_file_with_pipes(folderp / 'ec_with_pipes.txt')\n",
    "start = time.time()\n",
    "invoke_times = []\n",
    "for idx, criterion in enumerate(criterions):\n",
    "    idx += 1\n",
    "    invoke_start = time.time()\n",
    "    results = llm_chain.invoke(input={'criterion': criterion.value})\n",
    "    invoke_times.append(time.time() - invoke_start)\n",
    "    with open(folderp / f'{idx:02}_output.txt', 'w', encoding='utf-8') as fileout:\n",
    "        fileout.write(results['text'])\n",
    "    with open(folderp / f'{idx:02}_stats.yaml', 'w', encoding='utf-8') as fileout:\n",
    "        fileout.write(f\"\"\"elapsed_time: {int(time.time() - invoke_start)}s\n",
    "was_input_captured: {criterion.value in results['text']}\n",
    "original_text: '{criterion.value}'\n",
    "inclusion: {criterion.inclusion}\n",
    "\"\"\")\n",
    "with open(folderp / 'stats.yaml', 'w', encoding='utf-8') as fileout:\n",
    "    fileout.write(f\"\"\"total_time: {(time.time() - start) // 60}min\n",
    "avg_invoke_time: {sum(invoke_times) // len(invoke_times)}s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd962466-70d1-450f-8fee-37191949211d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dca61b6509c441ab15bb2dba7b49734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Copy the prompts that Langchain logs\n",
    "import json\n",
    "\n",
    "with open(f'{folder}/single_criterion_task_prompts.json') as filein:\n",
    "    prompts_used = json.load(filein)\n",
    "\n",
    "for idx, prompt_used in enumerate(prompts_used):\n",
    "    prompt_used = prompt_used['prompts'][0]\n",
    "    prompt_len = get_token_len(prompt_used)\n",
    "    with open(f'{folder}/single_criterion_task_{idx:02}_prompt.txt', 'w', encoding='utf-8') as fileout:\n",
    "        fileout.write(\"\"\"[metrics]\n",
    "length={prompt_len}\n",
    "\n",
    "[prompt]\n",
    "{prompt}\"\"\".format(prompt=prompt_used, prompt_len=prompt_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
