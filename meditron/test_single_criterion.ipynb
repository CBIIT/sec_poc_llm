{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34dddb4-6838-45ad-a805-2836af7ff6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e0db87bcab4e129b6011f419e8ffe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2434fa8e-0ebb-4b60-aa18-d305531a0ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    # stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=500,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    "    do_sample=True,\n",
    "    # streamer = transformers.TextStreamer(tokenizer)\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c14e5e4-8aa0-4140-9b5e-7cf26677dffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from prompt_examples.single_criterion_examples import prompt, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c99a8-f29f-44e0-a738-2977f72ec7a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(prompt.format(criterion=\"\"\"Patient must have undergone complete surgical resection of their stage IIA, IIB, IIIA or IIIB non-squamous or squamous b NSCLC per American Joint Committee on Cancer (AJCC) 8th edition and have had negative margins. N3 disease is not allowed.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17ef896-2a6a-4cf3-acdc-6b036b01a36f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  avg prompt: 121\n",
      "  min prompt: 66\n",
      "  max prompt: 227\n",
      "avg response: 89\n"
     ]
    }
   ],
   "source": [
    "from token_counting import *\n",
    "globalize_token_metrics(examples)\n",
    "print('  avg prompt:', AVG_PROMPT_LEN)\n",
    "print('  min prompt:', MIN_PROMPT_LEN)\n",
    "print('  max prompt:', MAX_PROMPT_LEN)\n",
    "print('avg response:', AVG_RES_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c02c6e5-e477-47be-a50c-5509444e292c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chunking import parse_file_with_pipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78706696-5748-4432-a898-82f9140f55d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "import time\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "langchain.debug = False\n",
    "langchain.verbose = False\n",
    "\n",
    "n = '01'\n",
    "folder = f'test_results_final/trial{n}'\n",
    "folderp = Path(folder)\n",
    "logfile = folderp / \"outputs_amended.log\"\n",
    "logger.add(logfile, colorize=False, enqueue=True)\n",
    "handler = langchain.callbacks.FileCallbackHandler(logfile)\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=False)\n",
    "\n",
    "criterions = parse_file_with_pipes(folderp / 'ec_with_pipes.txt')\n",
    "start = time.time()\n",
    "invoke_times = []\n",
    "for idx, criterion in enumerate(criterions):\n",
    "    idx += 1\n",
    "    if idx != 38:\n",
    "        continue\n",
    "    invoke_start = time.time()\n",
    "    results = llm_chain.invoke(input={'criterion': criterion.value})\n",
    "    invoke_times.append(time.time() - invoke_start)\n",
    "    with open(folderp / f'{idx:02}_output.txt', 'w', encoding='utf-8') as fileout:\n",
    "        fileout.write(results['text'])\n",
    "    with open(folderp / f'{idx:02}_stats.yaml', 'w', encoding='utf-8') as fileout:\n",
    "        fileout.write(f\"\"\"elapsed_time: {int(time.time() - invoke_start)}s\n",
    "was_input_captured: {criterion.value in results['text']}\n",
    "original_text: '{criterion.value}'\n",
    "inclusion: {criterion.inclusion}\n",
    "\"\"\")\n",
    "    if idx == 38:\n",
    "        break\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de873179-5723-4bd1-8aa3-256f827e90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folderp / 'stats.yaml', 'w', encoding='utf-8') as fileout:\n",
    "    fileout.write(f\"\"\"total_time: {(start - end) // 60}min\n",
    "avg_invoke_time: {sum(invoke_times) // len(invoke_times)}s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b635e145-9067-4828-a286-3c5a93ef8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing\n",
    "import glob\n",
    "import yaml\n",
    "\n",
    "output_files = glob.glob(str(folderp / '*_output.txt'))\n",
    "output_files.sort()\n",
    "for idx, file in enumerate(output_files):\n",
    "    idx += 1\n",
    "    stop_word = 'Criterion:'\n",
    "    lines = []\n",
    "    with open(file, encoding='utf-8') as filein:\n",
    "        original_text = False\n",
    "        for line in filein:\n",
    "            if line.startswith(stop_word):\n",
    "                break\n",
    "            if line == '\\n':\n",
    "                continue\n",
    "            if 'Original Text:' in line:\n",
    "                original_text = True\n",
    "                key = 'Original Text'\n",
    "                _, original_text_l1 = line.split(': ', maxsplit=1)\n",
    "                lines.append(key + ': |\\n')\n",
    "                lines.append('    ' + original_text_l1)\n",
    "            elif 'Disease/Condition:' in line:\n",
    "                original_text = False\n",
    "                lines.append(line.removeprefix('\\t').removeprefix('    '))\n",
    "            elif original_text:\n",
    "                lines.append('    ' + line)\n",
    "            else:\n",
    "                lines.append(line.removeprefix('\\t').removeprefix('    '))\n",
    "    output_reassembled = ''.join(lines)\n",
    "    with open(folderp / f'{idx:02}_output_cleaned.yaml', 'w', encoding='utf-8') as fileout:\n",
    "        fileout.write(output_reassembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e182cd8-b520-4ca4-8c8a-225efcaf9f66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_files_clean = glob.glob(str(folderp / '*_output_cleaned.yaml'))\n",
    "output_files_clean.sort()\n",
    "df = pd.DataFrame(columns=['Criterion Text In', 'Criterion Text Out', 'Inclusion/Exclusion', 'Disease', 'Biomarker', 'Procedure', 'Drug', 'Criterion Rule'])\n",
    "for idx, file in enumerate(output_files_clean):\n",
    "    idx += 1\n",
    "    with open(folderp / f'{idx:02}_stats.yaml', encoding='utf-8') as filein:\n",
    "        stats = yaml.safe_load(filein)\n",
    "    with open(file, encoding='utf-8') as filein:\n",
    "        print(file)\n",
    "        output = yaml.safe_load(filein)\n",
    "    if idx == 38:\n",
    "        output['Disease/Condition'] = None\n",
    "        output['Biomarker'] = None\n",
    "        output['Procedure'] = None\n",
    "        output['Drug'] = None\n",
    "        output['Computable Rule'] = None\n",
    "    row = {\n",
    "        'Criterion Text In': stats['original_text'],\n",
    "        'Criterion Text Out': output['Original Text'].strip(),\n",
    "        'Inclusion/Exclusion': 'Inclusion' if stats['inclusion'] else 'Exclusion',\n",
    "        'Disease': output['Disease/Condition'],\n",
    "        'Biomarker': output['Biomarker'],\n",
    "        'Procedure': output['Procedure'],\n",
    "        'Drug': output['Drug'],\n",
    "        'Criterion Rule': output['Computable Rule']\n",
    "    }\n",
    "    df = df.append(row, ignore_index=True)\n",
    "df = df.replace('none', None).replace('None', None)\n",
    "df.to_csv(folderp / 'results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
