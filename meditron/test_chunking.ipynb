{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04049c68-2e40-4a8b-9cf8-0810c2264b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021d0e2-364a-4abf-a3ef-02a75d4b949c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id=\"epfl-llm/meditron-7b\"\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    # token=token\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    # token=token\n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # token=token\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80efc1b6-8e3f-4f5f-b17f-65c8b0eceed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_token_len(text: str):\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba54b270-f43c-4917-bad2-519aa4ab9f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([29871,    13,    13], device='cuda:0'), tensor([29871,    13,    13,    13], device='cuda:0'), tensor([ 9330, 29901,    13, 21140,   340], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "stop_list = [\"\\n\\n\", \"\\n\\n\\n\", \"Task:\\nBelow\"]\n",
    "# stop_list = ['\\nHuman:', '\\n```\\n']\n",
    "stop_token_ids = [tokenizer(x, add_special_tokens=False)['input_ids'] for x in stop_list]\n",
    "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
    "print(stop_token_ids)\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba3167ac-85da-4355-b0a0-89cff695084d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=256,  # max number of tokens to generate in the output\n",
    "    # repetition_penalty=1.1,  # without this output begins repeating\n",
    "    do_sample=True,\n",
    "    streamer = transformers.TextStreamer(tokenizer)\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7985891-8fdb-4303-878b-fd1da5be075b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from prompt_examples.single_criterion_examples import examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "640268cf-85a8-4f50-aaf0-4ea9110a073d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # This is the list of examples available to select from.\n",
    "    examples,\n",
    "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    HuggingFaceEmbeddings(),\n",
    "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    Chroma,\n",
    "    # This is the number of examples to produce.\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"answer\"], template=\"Task:{context}{answer}\"\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"\"\"Task:\n",
    "Below is an example of clinical trial eligibility inclusion/exclusion criteria. Your task is to identify 3 categories of data within it. The 3 categories are: 1) Disease: a disorder affecting humans, 2) Biomarker: genes, proteins, or other substances that can be tested for to reveal important details about a patientâ€™s cancer, and 3) Prior Therapy: medications, surgeries, or procedures that a patient may be treated with. For each of the identified categories, state whether it is an inclusion or exclusion. Additionally, please summarize the criterion.\n",
    "Criteria:\n",
    "    {criteria}\n",
    "\"\"\",\n",
    "    input_variables=[\"criteria\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e53318-366a-4037-a594-371a80d2f86b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54f23e97-3728-4061-bb95-6b0b6842b3de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         avg: 227\n",
      "         min: 188\n",
      "         max: 252\n",
      "avg response: 61\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_res = 0\n",
    "minm = 2000\n",
    "maxm = 0\n",
    "for example in examples:\n",
    "    context_len = get_token_len(example['context'])\n",
    "    answer_len = get_token_len(example['answer'])\n",
    "    combined_len = context_len + answer_len\n",
    "    if combined_len < minm:\n",
    "        minm = combined_len\n",
    "    elif combined_len > maxm:\n",
    "        maxm = combined_len\n",
    "    total += combined_len\n",
    "    total_res += answer_len\n",
    "AVG_EXAMPLE_LEN = total // len(examples)\n",
    "MAX_W_SIZE = 2048\n",
    "AVG_RES_LEN = total_res // len(examples)\n",
    "print('         avg:', AVG_EXAMPLE_LEN)\n",
    "print('         min:', minm)\n",
    "print('         max:', maxm)\n",
    "print('avg response:', AVG_RES_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d423e3b-69ce-44cb-b451-b2aee94db13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ECDoc:\n",
    "    inc: list[str]\n",
    "    exc: list[str]\n",
    "    template = \"\"\"Inclusion Criteria\n",
    "{inclusion}\n",
    "\n",
    "Exclusion Criteria\n",
    "{exclusion}\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self, inc: list[str], exc: list[str]):\n",
    "        self.inc = inc\n",
    "        self.exc = exc\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return get_token_len(str(self))\n",
    "\n",
    "    def __str__(self) -> list[str]:\n",
    "        return self.template.format(inclusion=''.join(self.inc).rstrip(),\n",
    "                                   exclusion=''.join(self.exc).rstrip())\n",
    "\n",
    "    def split(self):\n",
    "        inc_len = len(self.inc)\n",
    "        exc_len = len(self.exc)\n",
    "        inc_midpoint = inc_len // 2\n",
    "        exc_midpoint = exc_len // 2\n",
    "        # Prevent Inclusion section from splitting too small\n",
    "        if inc_len <= 5:\n",
    "            inc_chunk_1 = self.inc\n",
    "            inc_chunk_2 = self.inc\n",
    "        else:\n",
    "            inc_chunk_1 = self.inc[:inc_midpoint]\n",
    "            inc_chunk_2 = self.inc[inc_midpoint:]\n",
    "        # Prevent Exclusion section from splitting too small\n",
    "        if exc_len <= 5:\n",
    "            exc_chunk_1 = self.exc\n",
    "            exc_chunk_2 = self.exc\n",
    "        else:\n",
    "            exc_chunk_1 = self.exc[:exc_midpoint]\n",
    "            exc_chunk_2 = self.exc[exc_midpoint:]\n",
    "        doc_chunk_1 = ECDoc(inc=inc_chunk_1, exc=exc_chunk_1)\n",
    "        doc_chunk_2 = ECDoc(inc=inc_chunk_2, exc=exc_chunk_2)\n",
    "        return doc_chunk_1, doc_chunk_2\n",
    "\n",
    "\n",
    "def parse_file(filename: str) -> ECDoc:\n",
    "    inc = []\n",
    "    exc = []\n",
    "    inclusion = True\n",
    "    with open(filename) as filein:\n",
    "        for line in filein.readlines():\n",
    "            if line.strip().startswith('Inclusion Criteria'):\n",
    "                continue\n",
    "            elif line.strip().startswith('Exclusion Criteria'):\n",
    "                inclusion = False\n",
    "                continue\n",
    "            elif line == '\\n':\n",
    "                continue\n",
    "            if inclusion:\n",
    "                inc.append(line)\n",
    "            else:\n",
    "                exc.append(line)\n",
    "    return ECDoc(inc=inc, exc=exc)\n",
    "\n",
    "\n",
    "def chunk_ec(doc: ECDoc) -> list[ECDoc]:\n",
    "    # Not too large\n",
    "    can_fit = doc.size + AVG_EXAMPLE_LEN < MAX_W_SIZE - AVG_RES_LEN\n",
    "    if can_fit:\n",
    "        return [doc]\n",
    "\n",
    "    last_pass_chunks = [doc]\n",
    "    while not can_fit:\n",
    "        new_chunks = []\n",
    "        for chunk in last_pass_chunks:\n",
    "            new_chunks.extend(chunk.split())\n",
    "        new_chunk_size = max([new_chunk.size for new_chunk in new_chunks])\n",
    "        can_fit = new_chunk_size + AVG_EXAMPLE_LEN < MAX_W_SIZE - AVG_RES_LEN\n",
    "        last_pass_chunks = new_chunks\n",
    "    return new_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c6b51-e531-4edc-bef9-2f400bf6244b",
   "metadata": {},
   "source": [
    "## Process and test trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fca9e975-e69c-4acd-8254-e02a8253357a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f0f21-47d3-4b94-a160-c731a54e7731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = '01'\n",
    "trial_folder = f'test_results/trial{n}'\n",
    "original_doc = parse_file(f'{trial_folder}/unstructured_ec.txt')\n",
    "\n",
    "# with open(f'{trial_folder}/inclusion_output', 'a') as fileout:\n",
    "for inc in original_doc.inc:\n",
    "    criteria = \"\"\"Inclusion Criteria\n",
    "{inclusion}\n",
    "\"\"\".format(inclusion=inc)\n",
    "    output = llm_chain.run(criteria=criteria)\n",
    "\n",
    "# for exc in original_doc.exc:\n",
    "#     criteria = \"\"\"Exclusion Criteria\n",
    "# {exclusion}\n",
    "# \"\"\".format(exclusion=exc)\n",
    "#     output = llm_chain.run(criteria=criteria)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
