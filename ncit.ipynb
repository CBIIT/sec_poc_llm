{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurusf = Path(\"Thesaurus.txt\")\n",
    "thesaurus_version = \"Thesaurus_24.02d\"\n",
    "\n",
    "if not thesaurusf.exists():\n",
    "    print(f\"Downloading {thesaurus_version}\")\n",
    "    !curl -O https://evs.nci.nih.gov/ftp1/NCI_Thesaurus/{thesaurus_version}.FLAT.zip\n",
    "    !unzip {thesaurus_version}.FLAT.zip\n",
    "\n",
    "ncit = pd.read_csv(thesaurusf, sep=\"\\t\", header=None, encoding=\"utf-8\")\n",
    "ncit.columns = [\n",
    "    \"code\",\n",
    "    \"concept IRI\",\n",
    "    \"parents\",\n",
    "    \"synonyms\",\n",
    "    \"definition\",\n",
    "    \"display name\",\n",
    "    \"concept status\",\n",
    "    \"semantic type\",\n",
    "    \"concept in subset\",\n",
    "]\n",
    "ncit = ncit.set_index(\"code\")\n",
    "display(ncit.head())\n",
    "ncit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Node mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row(code) -> pd.Series:\n",
    "    return ncit.loc[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODES_CACHE: dict[str, \"Node\"] = {}\n",
    "\n",
    "\n",
    "class Node:\n",
    "    code: str\n",
    "    parents: list[\"Node\"]\n",
    "    synonyms: set[str]\n",
    "    pref_name: str\n",
    "\n",
    "    def __init__(self, row: pd.Series) -> None:\n",
    "        self.code = row.name\n",
    "        self.parents = []\n",
    "        if pd.notna(row[\"parents\"]):\n",
    "            for parent_code in row[\"parents\"].split(\"|\"):\n",
    "                if parent_code not in NODES_CACHE:\n",
    "                    NODES_CACHE[parent_code] = Node(get_row(parent_code))\n",
    "                self.parents.append(NODES_CACHE[parent_code])\n",
    "        self.synonyms = set()\n",
    "        for synonym in row[\"synonyms\"].split(\"|\"):\n",
    "            self.synonyms.add(synonym)\n",
    "        self.pref_name = row[\"display name\"]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"<{self.code} <= {','.join(p.code for p in self.parents)}>\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<{self.code} <= {','.join(p.code for p in self.parents)}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parent_code, row in ncit.iterrows():\n",
    "    node = Node(row)\n",
    "    if parent_code not in NODES_CACHE:\n",
    "        NODES_CACHE[parent_code] = node\n",
    "len(NODES_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ncit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODES_CACHE['C5449']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traverse the Node mapping for a certain concept and children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "def get_children_of_code(nodes: dict[str, Node], parent_code: str):\n",
    "    children = set()\n",
    "    for c, n in nodes.items():\n",
    "        if parent_code in [p.code for p in n.parents]:\n",
    "            children.add(c)\n",
    "    return children\n",
    "\n",
    "\n",
    "def bfs(nodes: dict[str, Node], start_code: str):\n",
    "    visited = []  # Keep track of visited nodes.\n",
    "    queue = deque([start_code])  # Queue initialized with the start node code.\n",
    "    tracking_q = deque([(start_code, None)])\n",
    "\n",
    "    while queue:\n",
    "        current_code = queue.popleft()  # Dequeue a node code.\n",
    "        if current_code not in visited:\n",
    "            visited.append(current_code)\n",
    "\n",
    "            # Add all unvisited children to the queue.\n",
    "            node_children = get_children_of_code(nodes, current_code)\n",
    "            for child_code in node_children:\n",
    "                if child_code not in visited:\n",
    "                    queue.append(child_code)\n",
    "                if (child_code, current_code) not in tracking_q:\n",
    "                    tracking_q.append((child_code, current_code))\n",
    "\n",
    "    return visited, tracking_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neoplasm = \"C3262\"\n",
    "visited = bfs(NODES_CACHE, neoplasm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children, children_w_parents = visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the set of child2parents contains all unique pairs\n",
    "combos = set()\n",
    "for tup in children_w_parents:\n",
    "    combos.add(tup)\n",
    "assert len(combos) == len(children_w_parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for idx, (child, parent) in enumerate(children_w_parents):\n",
    "    if not parent:\n",
    "        assert child == neoplasm\n",
    "        continue\n",
    "    child, parent = NODES_CACHE[child], NODES_CACHE[parent]\n",
    "    for syn in child.synonyms:\n",
    "        data.append(\n",
    "            (idx + 1, syn, child.pref_name, child.code, parent.pref_name, parent.code)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"Level\",\n",
    "        \"Disease\",\n",
    "        \"Preferred Term\",\n",
    "        \"Code\",\n",
    "        \"Parent Term\",\n",
    "        \"Parent Term Code\",\n",
    "    ],\n",
    "    dtype=str\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output[\"Parent Term Code\"] == \"C5449\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call EVS API to get preferred terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "num_concepts_per_evs_call = 575\n",
    "concept_list = list(children)\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "\n",
    "concept_url_fstring = (\n",
    "    \"https://api-evsrest.nci.nih.gov/api/v1/concept/ncit?list=%s&include=summary\"\n",
    ")\n",
    "new_column_vals = []\n",
    "chunk_count = 0\n",
    "record_count = 0\n",
    "retry_limit = 3\n",
    "early_break = False\n",
    "\n",
    "pref_termsf = Path(\"preferred_terms.csv\")\n",
    "if not pref_termsf.exists():\n",
    "    print(\"Calling EVS to get preferred terms\")\n",
    "    for ch in chunks(concept_list, num_concepts_per_evs_call):\n",
    "        c_codes = list(ch)\n",
    "        record_count += len(c_codes)\n",
    "        c_codes_string = \",\".join(c_codes)\n",
    "        concept_url_string = concept_url_fstring % (c_codes_string)\n",
    "        retry_count = 0\n",
    "\n",
    "        while retry_count < retry_limit:\n",
    "            try:\n",
    "                r = requests.get(concept_url_string, timeout=(1.0, 15.0))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(\"exception -- \", e)\n",
    "                print(\"sleeping\")\n",
    "                retry_count += 1\n",
    "                if retry_count == retry_limit:\n",
    "                    print(\"retry max limit hit -- bailing out \")\n",
    "                    early_break = True\n",
    "                    break\n",
    "                time.sleep(15)\n",
    "            else:\n",
    "                concept_set = r.json()\n",
    "                for newc in concept_set:\n",
    "                    new_column_vals.append((newc[\"code\"], newc[\"name\"]))\n",
    "\n",
    "                chunk_count = chunk_count + 1\n",
    "                print(\n",
    "                    \"processing chunk \", chunk_count, \" record count = \", record_count\n",
    "                )\n",
    "                break\n",
    "        if early_break:\n",
    "            break\n",
    "\n",
    "    pref_terms_df = pd.DataFrame(data=new_column_vals, columns=[\"code\", \"pref_name\"])\n",
    "    pref_terms_df.to_csv(pref_termsf, index=False, encoding=\"utf-8\")\n",
    "else:\n",
    "    print(f\"Using saved {pref_termsf}\")\n",
    "    pref_terms_df = pd.read_csv(pref_termsf, encoding=\"utf-8\")\n",
    "\n",
    "pref_terms_df = pref_terms_df.set_index(\"code\")\n",
    "display(pref_terms_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"Preferred Term\"] = output.apply(\n",
    "    lambda row: row[\"Preferred Term\"]\n",
    "    if not pd.isna(row[\"Preferred Term\"])\n",
    "    else pref_terms_df.loc[row[\"Code\"]][\"pref_name\"],\n",
    "    axis=1,\n",
    ")\n",
    "output[\"Parent Term\"] = output.apply(\n",
    "    lambda row: row[\"Parent Term\"]\n",
    "    if not pd.isna(row[\"Parent Term\"])\n",
    "    else pref_terms_df.loc[row[\"Parent Term Code\"]][\"pref_name\"],\n",
    "    axis=1,\n",
    ")\n",
    "assert output[\"Preferred Term\"].hasnans is False\n",
    "assert output[\"Parent Term\"].hasnans is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease2code = output.loc[:, [\"Disease\", \"Code\"]].drop_duplicates()\n",
    "disease2code = disease2code.sort_values(by=['Disease'])\n",
    "disease2code.to_csv(\"disease-code.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "code2term = output.loc[:, [\"Code\", \"Preferred Term\"]].drop_duplicates()\n",
    "code2term = code2term.sort_values(by=['Code'])\n",
    "code2term.to_csv(\"code2term.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"neoplasm-concepts.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
