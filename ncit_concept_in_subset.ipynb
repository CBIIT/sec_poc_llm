{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurusf = Path(\"Thesaurus.txt\")\n",
    "thesaurus_version = \"Thesaurus_24.02d\"\n",
    "\n",
    "if not thesaurusf.exists():\n",
    "    print(f\"Downloading {thesaurus_version}\")\n",
    "    !curl -O https://evs.nci.nih.gov/ftp1/NCI_Thesaurus/{thesaurus_version}.FLAT.zip\n",
    "    !unzip {thesaurus_version}.FLAT.zip\n",
    "\n",
    "ncit = pd.read_csv(thesaurusf, sep=\"\\t\", header=None, encoding=\"utf-8\", dtype=str)\n",
    "ncit.columns = [\n",
    "    \"code\",\n",
    "    \"concept IRI\",\n",
    "    \"parents\",\n",
    "    \"synonyms\",\n",
    "    \"definition\",\n",
    "    \"display name\",\n",
    "    \"concept status\",\n",
    "    \"semantic type\",\n",
    "    \"concept in subset\",\n",
    "]\n",
    "ncit = ncit.set_index(\"code\")\n",
    "ncit = ncit.fillna('')\n",
    "display(ncit.head())\n",
    "ncit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the concepts to those in subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_keyword = \"biomarker\"\n",
    "subset_keyword = subset_keyword.lower()\n",
    "\n",
    "subset_term = \"CTRP Biomarker Terminology\"\n",
    "\n",
    "all_concepts_in_subset = ncit[\n",
    "    ncit[\"concept in subset\"].apply(\n",
    "        lambda concept_in_subset: subset_term.lower() in concept_in_subset.lower()\n",
    "    )\n",
    "]\n",
    "display(all_concepts_in_subset.head(), all_concepts_in_subset.shape)\n",
    "\n",
    "related_subset_terms = set()\n",
    "for concept_in_subset in ncit[\"concept in subset\"].unique():\n",
    "    for term in concept_in_subset.split(\"|\"):\n",
    "        if subset_keyword in term.lower():\n",
    "            related_subset_terms.add(term)\n",
    "related_subset_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for code, row in all_concepts_in_subset.iterrows():\n",
    "    parents = row['parents'].split('|')\n",
    "    assert parents\n",
    "    assert row['display name']\n",
    "    for parent in parents:\n",
    "        p_row = ncit.loc[parent]\n",
    "        synonyms = row[\"synonyms\"].split(\"|\")\n",
    "        for syn in synonyms:\n",
    "            data.append(\n",
    "                (\n",
    "                    syn,\n",
    "                    row[\"display name\"],\n",
    "                    code,\n",
    "                    p_row['display name'],\n",
    "                    parent\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"Term\",\n",
    "        \"Preferred Term\",\n",
    "        \"Code\",\n",
    "        \"Parent Term\",\n",
    "        \"Parent Term Code\",\n",
    "    ],\n",
    "    dtype=str\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call EVS API to get preferred terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "num_concepts_per_evs_call = 575\n",
    "concept_list = list(output['Parent Term Code'].unique())\n",
    "\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "\n",
    "concept_url_fstring = (\n",
    "    \"https://api-evsrest.nci.nih.gov/api/v1/concept/ncit?list=%s&include=summary\"\n",
    ")\n",
    "new_column_vals = []\n",
    "chunk_count = 0\n",
    "record_count = 0\n",
    "retry_limit = 3\n",
    "early_break = False\n",
    "\n",
    "pref_termsf = Path(f\"preferred_terms_{subset_keyword}.csv\")\n",
    "if not pref_termsf.exists():\n",
    "    print(\"Calling EVS to get preferred terms\")\n",
    "    for ch in chunks(concept_list, num_concepts_per_evs_call):\n",
    "        c_codes = list(ch)\n",
    "        record_count += len(c_codes)\n",
    "        c_codes_string = \",\".join(c_codes)\n",
    "        concept_url_string = concept_url_fstring % (c_codes_string)\n",
    "        retry_count = 0\n",
    "\n",
    "        while retry_count < retry_limit:\n",
    "            try:\n",
    "                r = requests.get(concept_url_string, timeout=(1.0, 15.0))\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(\"exception -- \", e)\n",
    "                print(\"sleeping\")\n",
    "                retry_count += 1\n",
    "                if retry_count == retry_limit:\n",
    "                    print(\"retry max limit hit -- bailing out \")\n",
    "                    early_break = True\n",
    "                    break\n",
    "                time.sleep(15)\n",
    "            else:\n",
    "                concept_set = r.json()\n",
    "                for newc in concept_set:\n",
    "                    new_column_vals.append((newc[\"code\"], newc[\"name\"]))\n",
    "\n",
    "                chunk_count = chunk_count + 1\n",
    "                print(\n",
    "                    \"processing chunk \", chunk_count, \" record count = \", record_count\n",
    "                )\n",
    "                break\n",
    "        if early_break:\n",
    "            break\n",
    "\n",
    "    pref_terms_df = pd.DataFrame(data=new_column_vals, columns=[\"code\", \"pref_name\"])\n",
    "    pref_terms_df.to_csv(pref_termsf, index=False, encoding=\"utf-8\")\n",
    "else:\n",
    "    print(f\"Using saved {pref_termsf}\")\n",
    "    pref_terms_df = pd.read_csv(pref_termsf, encoding=\"utf-8\")\n",
    "\n",
    "pref_terms_df = pref_terms_df.set_index(\"code\")\n",
    "display(pref_terms_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output[\"Preferred Term\"] = output.apply(\n",
    "#     lambda row: row[\"Preferred Term\"]\n",
    "#     if not pd.isna(row[\"Preferred Term\"])\n",
    "#     else pref_terms_df.loc[row[\"Code\"]][\"pref_name\"],\n",
    "#     axis=1,\n",
    "# )\n",
    "output[\"Parent Term\"] = output.apply(\n",
    "    lambda row: row[\"Parent Term\"]\n",
    "    if row[\"Parent Term\"]\n",
    "    else pref_terms_df.loc[row[\"Parent Term Code\"]][\"pref_name\"],\n",
    "    axis=1,\n",
    ")\n",
    "assert output[\"Preferred Term\"].all()\n",
    "assert output[\"Parent Term\"].all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term2code = output.loc[:, [\"Term\", \"Code\"]].drop_duplicates()\n",
    "term2code = term2code.sort_values(by=['Term'])\n",
    "term2code.to_csv(\"term-code.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "code2pref_term = output.loc[:, [\"Code\", \"Preferred Term\"]].drop_duplicates()\n",
    "code2pref_term = code2pref_term.sort_values(by=['Code'])\n",
    "code2pref_term.to_csv(\"code2term.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "output.to_csv(f\"ncit-{subset_keyword}-concepts.tsv\", sep=\"\\t\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
